
=== Vector Loads and Stores Intrinsics

[[policy-variant-overloadedvector-unit-stride-load]]
==== Vector Unit-Stride Load Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vle16_tu(vfloat16mf4_t vd, const _Float16 *rs1,
                               size_t vl);
vfloat16mf2_t __riscv_vle16_tu(vfloat16mf2_t vd, const _Float16 *rs1,
                               size_t vl);
vfloat16m1_t __riscv_vle16_tu(vfloat16m1_t vd, const _Float16 *rs1, size_t vl);
vfloat16m2_t __riscv_vle16_tu(vfloat16m2_t vd, const _Float16 *rs1, size_t vl);
vfloat16m4_t __riscv_vle16_tu(vfloat16m4_t vd, const _Float16 *rs1, size_t vl);
vfloat16m8_t __riscv_vle16_tu(vfloat16m8_t vd, const _Float16 *rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vle16_tum(vbool64_t vm, vfloat16mf4_t vd,
                                const _Float16 *rs1, size_t vl);
vfloat16mf2_t __riscv_vle16_tum(vbool32_t vm, vfloat16mf2_t vd,
                                const _Float16 *rs1, size_t vl);
vfloat16m1_t __riscv_vle16_tum(vbool16_t vm, vfloat16m1_t vd,
                               const _Float16 *rs1, size_t vl);
vfloat16m2_t __riscv_vle16_tum(vbool8_t vm, vfloat16m2_t vd,
                               const _Float16 *rs1, size_t vl);
vfloat16m4_t __riscv_vle16_tum(vbool4_t vm, vfloat16m4_t vd,
                               const _Float16 *rs1, size_t vl);
vfloat16m8_t __riscv_vle16_tum(vbool2_t vm, vfloat16m8_t vd,
                               const _Float16 *rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vle16_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                 const _Float16 *rs1, size_t vl);
vfloat16mf2_t __riscv_vle16_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                 const _Float16 *rs1, size_t vl);
vfloat16m1_t __riscv_vle16_tumu(vbool16_t vm, vfloat16m1_t vd,
                                const _Float16 *rs1, size_t vl);
vfloat16m2_t __riscv_vle16_tumu(vbool8_t vm, vfloat16m2_t vd,
                                const _Float16 *rs1, size_t vl);
vfloat16m4_t __riscv_vle16_tumu(vbool4_t vm, vfloat16m4_t vd,
                                const _Float16 *rs1, size_t vl);
vfloat16m8_t __riscv_vle16_tumu(vbool2_t vm, vfloat16m8_t vd,
                                const _Float16 *rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vle16_mu(vbool64_t vm, vfloat16mf4_t vd,
                               const _Float16 *rs1, size_t vl);
vfloat16mf2_t __riscv_vle16_mu(vbool32_t vm, vfloat16mf2_t vd,
                               const _Float16 *rs1, size_t vl);
vfloat16m1_t __riscv_vle16_mu(vbool16_t vm, vfloat16m1_t vd,
                              const _Float16 *rs1, size_t vl);
vfloat16m2_t __riscv_vle16_mu(vbool8_t vm, vfloat16m2_t vd, const _Float16 *rs1,
                              size_t vl);
vfloat16m4_t __riscv_vle16_mu(vbool4_t vm, vfloat16m4_t vd, const _Float16 *rs1,
                              size_t vl);
vfloat16m8_t __riscv_vle16_mu(vbool2_t vm, vfloat16m8_t vd, const _Float16 *rs1,
                              size_t vl);
----

[[policy-variant-overloadedvector-unit-stride-store]]
==== Vector Unit-Stride Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-strided-load]]
==== Vector Strided Load Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vlse16_tu(vfloat16mf4_t vd, const _Float16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vfloat16mf2_t __riscv_vlse16_tu(vfloat16mf2_t vd, const _Float16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vfloat16m1_t __riscv_vlse16_tu(vfloat16m1_t vd, const _Float16 *rs1,
                               ptrdiff_t rs2, size_t vl);
vfloat16m2_t __riscv_vlse16_tu(vfloat16m2_t vd, const _Float16 *rs1,
                               ptrdiff_t rs2, size_t vl);
vfloat16m4_t __riscv_vlse16_tu(vfloat16m4_t vd, const _Float16 *rs1,
                               ptrdiff_t rs2, size_t vl);
vfloat16m8_t __riscv_vlse16_tu(vfloat16m8_t vd, const _Float16 *rs1,
                               ptrdiff_t rs2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vlse16_tum(vbool64_t vm, vfloat16mf4_t vd,
                                 const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16mf2_t __riscv_vlse16_tum(vbool32_t vm, vfloat16mf2_t vd,
                                 const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m1_t __riscv_vlse16_tum(vbool16_t vm, vfloat16m1_t vd,
                                const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m2_t __riscv_vlse16_tum(vbool8_t vm, vfloat16m2_t vd,
                                const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m4_t __riscv_vlse16_tum(vbool4_t vm, vfloat16m4_t vd,
                                const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m8_t __riscv_vlse16_tum(vbool2_t vm, vfloat16m8_t vd,
                                const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vlse16_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                  const _Float16 *rs1, ptrdiff_t rs2,
                                  size_t vl);
vfloat16mf2_t __riscv_vlse16_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                  const _Float16 *rs1, ptrdiff_t rs2,
                                  size_t vl);
vfloat16m1_t __riscv_vlse16_tumu(vbool16_t vm, vfloat16m1_t vd,
                                 const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m2_t __riscv_vlse16_tumu(vbool8_t vm, vfloat16m2_t vd,
                                 const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m4_t __riscv_vlse16_tumu(vbool4_t vm, vfloat16m4_t vd,
                                 const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m8_t __riscv_vlse16_tumu(vbool2_t vm, vfloat16m8_t vd,
                                 const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vlse16_mu(vbool64_t vm, vfloat16mf4_t vd,
                                const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16mf2_t __riscv_vlse16_mu(vbool32_t vm, vfloat16mf2_t vd,
                                const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m1_t __riscv_vlse16_mu(vbool16_t vm, vfloat16m1_t vd,
                               const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m2_t __riscv_vlse16_mu(vbool8_t vm, vfloat16m2_t vd,
                               const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m4_t __riscv_vlse16_mu(vbool4_t vm, vfloat16m4_t vd,
                               const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
vfloat16m8_t __riscv_vlse16_mu(vbool2_t vm, vfloat16m8_t vd,
                               const _Float16 *rs1, ptrdiff_t rs2, size_t vl);
----

[[policy-variant-overloadedvector-strided-store]]
==== Vector Strided Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-indexed-load]]
==== Vector Indexed Load Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vloxei16_tu(vfloat16mf4_t vd, const _Float16 *rs1,
                                  vuint16mf4_t rs2, size_t vl);
vfloat16mf2_t __riscv_vloxei16_tu(vfloat16mf2_t vd, const _Float16 *rs1,
                                  vuint16mf2_t rs2, size_t vl);
vfloat16m1_t __riscv_vloxei16_tu(vfloat16m1_t vd, const _Float16 *rs1,
                                 vuint16m1_t rs2, size_t vl);
vfloat16m2_t __riscv_vloxei16_tu(vfloat16m2_t vd, const _Float16 *rs1,
                                 vuint16m2_t rs2, size_t vl);
vfloat16m4_t __riscv_vloxei16_tu(vfloat16m4_t vd, const _Float16 *rs1,
                                 vuint16m4_t rs2, size_t vl);
vfloat16m8_t __riscv_vloxei16_tu(vfloat16m8_t vd, const _Float16 *rs1,
                                 vuint16m8_t rs2, size_t vl);
vfloat16mf4_t __riscv_vluxei16_tu(vfloat16mf4_t vd, const _Float16 *rs1,
                                  vuint16mf4_t rs2, size_t vl);
vfloat16mf2_t __riscv_vluxei16_tu(vfloat16mf2_t vd, const _Float16 *rs1,
                                  vuint16mf2_t rs2, size_t vl);
vfloat16m1_t __riscv_vluxei16_tu(vfloat16m1_t vd, const _Float16 *rs1,
                                 vuint16m1_t rs2, size_t vl);
vfloat16m2_t __riscv_vluxei16_tu(vfloat16m2_t vd, const _Float16 *rs1,
                                 vuint16m2_t rs2, size_t vl);
vfloat16m4_t __riscv_vluxei16_tu(vfloat16m4_t vd, const _Float16 *rs1,
                                 vuint16m4_t rs2, size_t vl);
vfloat16m8_t __riscv_vluxei16_tu(vfloat16m8_t vd, const _Float16 *rs1,
                                 vuint16m8_t rs2, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vloxei16_tum(vbool64_t vm, vfloat16mf4_t vd,
                                   const _Float16 *rs1, vuint16mf4_t rs2,
                                   size_t vl);
vfloat16mf2_t __riscv_vloxei16_tum(vbool32_t vm, vfloat16mf2_t vd,
                                   const _Float16 *rs1, vuint16mf2_t rs2,
                                   size_t vl);
vfloat16m1_t __riscv_vloxei16_tum(vbool16_t vm, vfloat16m1_t vd,
                                  const _Float16 *rs1, vuint16m1_t rs2,
                                  size_t vl);
vfloat16m2_t __riscv_vloxei16_tum(vbool8_t vm, vfloat16m2_t vd,
                                  const _Float16 *rs1, vuint16m2_t rs2,
                                  size_t vl);
vfloat16m4_t __riscv_vloxei16_tum(vbool4_t vm, vfloat16m4_t vd,
                                  const _Float16 *rs1, vuint16m4_t rs2,
                                  size_t vl);
vfloat16m8_t __riscv_vloxei16_tum(vbool2_t vm, vfloat16m8_t vd,
                                  const _Float16 *rs1, vuint16m8_t rs2,
                                  size_t vl);
vfloat16mf4_t __riscv_vluxei16_tum(vbool64_t vm, vfloat16mf4_t vd,
                                   const _Float16 *rs1, vuint16mf4_t rs2,
                                   size_t vl);
vfloat16mf2_t __riscv_vluxei16_tum(vbool32_t vm, vfloat16mf2_t vd,
                                   const _Float16 *rs1, vuint16mf2_t rs2,
                                   size_t vl);
vfloat16m1_t __riscv_vluxei16_tum(vbool16_t vm, vfloat16m1_t vd,
                                  const _Float16 *rs1, vuint16m1_t rs2,
                                  size_t vl);
vfloat16m2_t __riscv_vluxei16_tum(vbool8_t vm, vfloat16m2_t vd,
                                  const _Float16 *rs1, vuint16m2_t rs2,
                                  size_t vl);
vfloat16m4_t __riscv_vluxei16_tum(vbool4_t vm, vfloat16m4_t vd,
                                  const _Float16 *rs1, vuint16m4_t rs2,
                                  size_t vl);
vfloat16m8_t __riscv_vluxei16_tum(vbool2_t vm, vfloat16m8_t vd,
                                  const _Float16 *rs1, vuint16m8_t rs2,
                                  size_t vl);
// masked functions
vfloat16mf4_t __riscv_vloxei16_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                    const _Float16 *rs1, vuint16mf4_t rs2,
                                    size_t vl);
vfloat16mf2_t __riscv_vloxei16_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                    const _Float16 *rs1, vuint16mf2_t rs2,
                                    size_t vl);
vfloat16m1_t __riscv_vloxei16_tumu(vbool16_t vm, vfloat16m1_t vd,
                                   const _Float16 *rs1, vuint16m1_t rs2,
                                   size_t vl);
vfloat16m2_t __riscv_vloxei16_tumu(vbool8_t vm, vfloat16m2_t vd,
                                   const _Float16 *rs1, vuint16m2_t rs2,
                                   size_t vl);
vfloat16m4_t __riscv_vloxei16_tumu(vbool4_t vm, vfloat16m4_t vd,
                                   const _Float16 *rs1, vuint16m4_t rs2,
                                   size_t vl);
vfloat16m8_t __riscv_vloxei16_tumu(vbool2_t vm, vfloat16m8_t vd,
                                   const _Float16 *rs1, vuint16m8_t rs2,
                                   size_t vl);
vfloat16mf4_t __riscv_vluxei16_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                    const _Float16 *rs1, vuint16mf4_t rs2,
                                    size_t vl);
vfloat16mf2_t __riscv_vluxei16_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                    const _Float16 *rs1, vuint16mf2_t rs2,
                                    size_t vl);
vfloat16m1_t __riscv_vluxei16_tumu(vbool16_t vm, vfloat16m1_t vd,
                                   const _Float16 *rs1, vuint16m1_t rs2,
                                   size_t vl);
vfloat16m2_t __riscv_vluxei16_tumu(vbool8_t vm, vfloat16m2_t vd,
                                   const _Float16 *rs1, vuint16m2_t rs2,
                                   size_t vl);
vfloat16m4_t __riscv_vluxei16_tumu(vbool4_t vm, vfloat16m4_t vd,
                                   const _Float16 *rs1, vuint16m4_t rs2,
                                   size_t vl);
vfloat16m8_t __riscv_vluxei16_tumu(vbool2_t vm, vfloat16m8_t vd,
                                   const _Float16 *rs1, vuint16m8_t rs2,
                                   size_t vl);
// masked functions
vfloat16mf4_t __riscv_vloxei16_mu(vbool64_t vm, vfloat16mf4_t vd,
                                  const _Float16 *rs1, vuint16mf4_t rs2,
                                  size_t vl);
vfloat16mf2_t __riscv_vloxei16_mu(vbool32_t vm, vfloat16mf2_t vd,
                                  const _Float16 *rs1, vuint16mf2_t rs2,
                                  size_t vl);
vfloat16m1_t __riscv_vloxei16_mu(vbool16_t vm, vfloat16m1_t vd,
                                 const _Float16 *rs1, vuint16m1_t rs2,
                                 size_t vl);
vfloat16m2_t __riscv_vloxei16_mu(vbool8_t vm, vfloat16m2_t vd,
                                 const _Float16 *rs1, vuint16m2_t rs2,
                                 size_t vl);
vfloat16m4_t __riscv_vloxei16_mu(vbool4_t vm, vfloat16m4_t vd,
                                 const _Float16 *rs1, vuint16m4_t rs2,
                                 size_t vl);
vfloat16m8_t __riscv_vloxei16_mu(vbool2_t vm, vfloat16m8_t vd,
                                 const _Float16 *rs1, vuint16m8_t rs2,
                                 size_t vl);
vfloat16mf4_t __riscv_vluxei16_mu(vbool64_t vm, vfloat16mf4_t vd,
                                  const _Float16 *rs1, vuint16mf4_t rs2,
                                  size_t vl);
vfloat16mf2_t __riscv_vluxei16_mu(vbool32_t vm, vfloat16mf2_t vd,
                                  const _Float16 *rs1, vuint16mf2_t rs2,
                                  size_t vl);
vfloat16m1_t __riscv_vluxei16_mu(vbool16_t vm, vfloat16m1_t vd,
                                 const _Float16 *rs1, vuint16m1_t rs2,
                                 size_t vl);
vfloat16m2_t __riscv_vluxei16_mu(vbool8_t vm, vfloat16m2_t vd,
                                 const _Float16 *rs1, vuint16m2_t rs2,
                                 size_t vl);
vfloat16m4_t __riscv_vluxei16_mu(vbool4_t vm, vfloat16m4_t vd,
                                 const _Float16 *rs1, vuint16m4_t rs2,
                                 size_t vl);
vfloat16m8_t __riscv_vluxei16_mu(vbool2_t vm, vfloat16m8_t vd,
                                 const _Float16 *rs1, vuint16m8_t rs2,
                                 size_t vl);
----

[[policy-variant-overloadedvector-indexed-store]]
==== Vector Indexed Store Intrinsics
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedunit-stride-fault-only-first-loads]]
==== Unit-stride Fault-Only-First Loads Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vle16ff_tu(vfloat16mf4_t vd, const _Float16 *rs1,
                                 size_t *new_vl, size_t vl);
vfloat16mf2_t __riscv_vle16ff_tu(vfloat16mf2_t vd, const _Float16 *rs1,
                                 size_t *new_vl, size_t vl);
vfloat16m1_t __riscv_vle16ff_tu(vfloat16m1_t vd, const _Float16 *rs1,
                                size_t *new_vl, size_t vl);
vfloat16m2_t __riscv_vle16ff_tu(vfloat16m2_t vd, const _Float16 *rs1,
                                size_t *new_vl, size_t vl);
vfloat16m4_t __riscv_vle16ff_tu(vfloat16m4_t vd, const _Float16 *rs1,
                                size_t *new_vl, size_t vl);
vfloat16m8_t __riscv_vle16ff_tu(vfloat16m8_t vd, const _Float16 *rs1,
                                size_t *new_vl, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vle16ff_tum(vbool64_t vm, vfloat16mf4_t vd,
                                  const _Float16 *rs1, size_t *new_vl,
                                  size_t vl);
vfloat16mf2_t __riscv_vle16ff_tum(vbool32_t vm, vfloat16mf2_t vd,
                                  const _Float16 *rs1, size_t *new_vl,
                                  size_t vl);
vfloat16m1_t __riscv_vle16ff_tum(vbool16_t vm, vfloat16m1_t vd,
                                 const _Float16 *rs1, size_t *new_vl,
                                 size_t vl);
vfloat16m2_t __riscv_vle16ff_tum(vbool8_t vm, vfloat16m2_t vd,
                                 const _Float16 *rs1, size_t *new_vl,
                                 size_t vl);
vfloat16m4_t __riscv_vle16ff_tum(vbool4_t vm, vfloat16m4_t vd,
                                 const _Float16 *rs1, size_t *new_vl,
                                 size_t vl);
vfloat16m8_t __riscv_vle16ff_tum(vbool2_t vm, vfloat16m8_t vd,
                                 const _Float16 *rs1, size_t *new_vl,
                                 size_t vl);
// masked functions
vfloat16mf4_t __riscv_vle16ff_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                   const _Float16 *rs1, size_t *new_vl,
                                   size_t vl);
vfloat16mf2_t __riscv_vle16ff_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                   const _Float16 *rs1, size_t *new_vl,
                                   size_t vl);
vfloat16m1_t __riscv_vle16ff_tumu(vbool16_t vm, vfloat16m1_t vd,
                                  const _Float16 *rs1, size_t *new_vl,
                                  size_t vl);
vfloat16m2_t __riscv_vle16ff_tumu(vbool8_t vm, vfloat16m2_t vd,
                                  const _Float16 *rs1, size_t *new_vl,
                                  size_t vl);
vfloat16m4_t __riscv_vle16ff_tumu(vbool4_t vm, vfloat16m4_t vd,
                                  const _Float16 *rs1, size_t *new_vl,
                                  size_t vl);
vfloat16m8_t __riscv_vle16ff_tumu(vbool2_t vm, vfloat16m8_t vd,
                                  const _Float16 *rs1, size_t *new_vl,
                                  size_t vl);
// masked functions
vfloat16mf4_t __riscv_vle16ff_mu(vbool64_t vm, vfloat16mf4_t vd,
                                 const _Float16 *rs1, size_t *new_vl,
                                 size_t vl);
vfloat16mf2_t __riscv_vle16ff_mu(vbool32_t vm, vfloat16mf2_t vd,
                                 const _Float16 *rs1, size_t *new_vl,
                                 size_t vl);
vfloat16m1_t __riscv_vle16ff_mu(vbool16_t vm, vfloat16m1_t vd,
                                const _Float16 *rs1, size_t *new_vl, size_t vl);
vfloat16m2_t __riscv_vle16ff_mu(vbool8_t vm, vfloat16m2_t vd,
                                const _Float16 *rs1, size_t *new_vl, size_t vl);
vfloat16m4_t __riscv_vle16ff_mu(vbool4_t vm, vfloat16m4_t vd,
                                const _Float16 *rs1, size_t *new_vl, size_t vl);
vfloat16m8_t __riscv_vle16ff_mu(vbool2_t vm, vfloat16m8_t vd,
                                const _Float16 *rs1, size_t *new_vl, size_t vl);
----

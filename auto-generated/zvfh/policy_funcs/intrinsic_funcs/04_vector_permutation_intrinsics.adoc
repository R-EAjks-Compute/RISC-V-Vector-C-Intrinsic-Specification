
=== Vector Permutation Intrinsics

[[policy-variant-scalar-move]]
==== Integer and Floating-Point Scalar Move Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vfmv_s_f_f16mf4_tu(vfloat16mf4_t vd, _Float16 rs1,
                                         size_t vl);
vfloat16mf2_t __riscv_vfmv_s_f_f16mf2_tu(vfloat16mf2_t vd, _Float16 rs1,
                                         size_t vl);
vfloat16m1_t __riscv_vfmv_s_f_f16m1_tu(vfloat16m1_t vd, _Float16 rs1,
                                       size_t vl);
vfloat16m2_t __riscv_vfmv_s_f_f16m2_tu(vfloat16m2_t vd, _Float16 rs1,
                                       size_t vl);
vfloat16m4_t __riscv_vfmv_s_f_f16m4_tu(vfloat16m4_t vd, _Float16 rs1,
                                       size_t vl);
vfloat16m8_t __riscv_vfmv_s_f_f16m8_tu(vfloat16m8_t vd, _Float16 rs1,
                                       size_t vl);
----

[[policy-variant-vector-slideup]]
==== Vector Slideup Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vslideup_vx_f16mf4_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                            size_t rs1, size_t vl);
vfloat16mf2_t __riscv_vslideup_vx_f16mf2_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                            size_t rs1, size_t vl);
vfloat16m1_t __riscv_vslideup_vx_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                          size_t rs1, size_t vl);
vfloat16m2_t __riscv_vslideup_vx_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                          size_t rs1, size_t vl);
vfloat16m4_t __riscv_vslideup_vx_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                          size_t rs1, size_t vl);
vfloat16m8_t __riscv_vslideup_vx_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                          size_t rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_vx_f16mf4_tum(vbool64_t vm, vfloat16mf4_t vd,
                                             vfloat16mf4_t vs2, size_t rs1,
                                             size_t vl);
vfloat16mf2_t __riscv_vslideup_vx_f16mf2_tum(vbool32_t vm, vfloat16mf2_t vd,
                                             vfloat16mf2_t vs2, size_t rs1,
                                             size_t vl);
vfloat16m1_t __riscv_vslideup_vx_f16m1_tum(vbool16_t vm, vfloat16m1_t vd,
                                           vfloat16m1_t vs2, size_t rs1,
                                           size_t vl);
vfloat16m2_t __riscv_vslideup_vx_f16m2_tum(vbool8_t vm, vfloat16m2_t vd,
                                           vfloat16m2_t vs2, size_t rs1,
                                           size_t vl);
vfloat16m4_t __riscv_vslideup_vx_f16m4_tum(vbool4_t vm, vfloat16m4_t vd,
                                           vfloat16m4_t vs2, size_t rs1,
                                           size_t vl);
vfloat16m8_t __riscv_vslideup_vx_f16m8_tum(vbool2_t vm, vfloat16m8_t vd,
                                           vfloat16m8_t vs2, size_t rs1,
                                           size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_vx_f16mf4_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2, size_t rs1,
                                              size_t vl);
vfloat16mf2_t __riscv_vslideup_vx_f16mf2_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2, size_t rs1,
                                              size_t vl);
vfloat16m1_t __riscv_vslideup_vx_f16m1_tumu(vbool16_t vm, vfloat16m1_t vd,
                                            vfloat16m1_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m2_t __riscv_vslideup_vx_f16m2_tumu(vbool8_t vm, vfloat16m2_t vd,
                                            vfloat16m2_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m4_t __riscv_vslideup_vx_f16m4_tumu(vbool4_t vm, vfloat16m4_t vd,
                                            vfloat16m4_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m8_t __riscv_vslideup_vx_f16m8_tumu(vbool2_t vm, vfloat16m8_t vd,
                                            vfloat16m8_t vs2, size_t rs1,
                                            size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_vx_f16mf4_mu(vbool64_t vm, vfloat16mf4_t vd,
                                            vfloat16mf4_t vs2, size_t rs1,
                                            size_t vl);
vfloat16mf2_t __riscv_vslideup_vx_f16mf2_mu(vbool32_t vm, vfloat16mf2_t vd,
                                            vfloat16mf2_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m1_t __riscv_vslideup_vx_f16m1_mu(vbool16_t vm, vfloat16m1_t vd,
                                          vfloat16m1_t vs2, size_t rs1,
                                          size_t vl);
vfloat16m2_t __riscv_vslideup_vx_f16m2_mu(vbool8_t vm, vfloat16m2_t vd,
                                          vfloat16m2_t vs2, size_t rs1,
                                          size_t vl);
vfloat16m4_t __riscv_vslideup_vx_f16m4_mu(vbool4_t vm, vfloat16m4_t vd,
                                          vfloat16m4_t vs2, size_t rs1,
                                          size_t vl);
vfloat16m8_t __riscv_vslideup_vx_f16m8_mu(vbool2_t vm, vfloat16m8_t vd,
                                          vfloat16m8_t vs2, size_t rs1,
                                          size_t vl);
----

[[policy-variant-vector-slidedown]]
==== Vector Slidedown Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vslidedown_vx_f16mf4_tu(vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2, size_t rs1,
                                              size_t vl);
vfloat16mf2_t __riscv_vslidedown_vx_f16mf2_tu(vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2, size_t rs1,
                                              size_t vl);
vfloat16m1_t __riscv_vslidedown_vx_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                            size_t rs1, size_t vl);
vfloat16m2_t __riscv_vslidedown_vx_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                            size_t rs1, size_t vl);
vfloat16m4_t __riscv_vslidedown_vx_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                            size_t rs1, size_t vl);
vfloat16m8_t __riscv_vslidedown_vx_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                            size_t rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_vx_f16mf4_tum(vbool64_t vm, vfloat16mf4_t vd,
                                               vfloat16mf4_t vs2, size_t rs1,
                                               size_t vl);
vfloat16mf2_t __riscv_vslidedown_vx_f16mf2_tum(vbool32_t vm, vfloat16mf2_t vd,
                                               vfloat16mf2_t vs2, size_t rs1,
                                               size_t vl);
vfloat16m1_t __riscv_vslidedown_vx_f16m1_tum(vbool16_t vm, vfloat16m1_t vd,
                                             vfloat16m1_t vs2, size_t rs1,
                                             size_t vl);
vfloat16m2_t __riscv_vslidedown_vx_f16m2_tum(vbool8_t vm, vfloat16m2_t vd,
                                             vfloat16m2_t vs2, size_t rs1,
                                             size_t vl);
vfloat16m4_t __riscv_vslidedown_vx_f16m4_tum(vbool4_t vm, vfloat16m4_t vd,
                                             vfloat16m4_t vs2, size_t rs1,
                                             size_t vl);
vfloat16m8_t __riscv_vslidedown_vx_f16m8_tum(vbool2_t vm, vfloat16m8_t vd,
                                             vfloat16m8_t vs2, size_t rs1,
                                             size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_vx_f16mf4_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                                vfloat16mf4_t vs2, size_t rs1,
                                                size_t vl);
vfloat16mf2_t __riscv_vslidedown_vx_f16mf2_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                                vfloat16mf2_t vs2, size_t rs1,
                                                size_t vl);
vfloat16m1_t __riscv_vslidedown_vx_f16m1_tumu(vbool16_t vm, vfloat16m1_t vd,
                                              vfloat16m1_t vs2, size_t rs1,
                                              size_t vl);
vfloat16m2_t __riscv_vslidedown_vx_f16m2_tumu(vbool8_t vm, vfloat16m2_t vd,
                                              vfloat16m2_t vs2, size_t rs1,
                                              size_t vl);
vfloat16m4_t __riscv_vslidedown_vx_f16m4_tumu(vbool4_t vm, vfloat16m4_t vd,
                                              vfloat16m4_t vs2, size_t rs1,
                                              size_t vl);
vfloat16m8_t __riscv_vslidedown_vx_f16m8_tumu(vbool2_t vm, vfloat16m8_t vd,
                                              vfloat16m8_t vs2, size_t rs1,
                                              size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_vx_f16mf4_mu(vbool64_t vm, vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2, size_t rs1,
                                              size_t vl);
vfloat16mf2_t __riscv_vslidedown_vx_f16mf2_mu(vbool32_t vm, vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2, size_t rs1,
                                              size_t vl);
vfloat16m1_t __riscv_vslidedown_vx_f16m1_mu(vbool16_t vm, vfloat16m1_t vd,
                                            vfloat16m1_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m2_t __riscv_vslidedown_vx_f16m2_mu(vbool8_t vm, vfloat16m2_t vd,
                                            vfloat16m2_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m4_t __riscv_vslidedown_vx_f16m4_mu(vbool4_t vm, vfloat16m4_t vd,
                                            vfloat16m4_t vs2, size_t rs1,
                                            size_t vl);
vfloat16m8_t __riscv_vslidedown_vx_f16m8_mu(vbool2_t vm, vfloat16m8_t vd,
                                            vfloat16m8_t vs2, size_t rs1,
                                            size_t vl);
----

[[policy-variant-vector-slide1up-and-slide1down]]
==== Vector Slide1up and Slide1down Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vfslide1up_vf_f16mf4_tu(vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16mf2_t __riscv_vfslide1up_vf_f16mf2_tu(vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m1_t __riscv_vfslide1up_vf_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                            _Float16 rs1, size_t vl);
vfloat16m2_t __riscv_vfslide1up_vf_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                            _Float16 rs1, size_t vl);
vfloat16m4_t __riscv_vfslide1up_vf_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                            _Float16 rs1, size_t vl);
vfloat16m8_t __riscv_vfslide1up_vf_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                            _Float16 rs1, size_t vl);
vfloat16mf4_t __riscv_vfslide1down_vf_f16mf4_tu(vfloat16mf4_t vd,
                                                vfloat16mf4_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16mf2_t __riscv_vfslide1down_vf_f16mf2_tu(vfloat16mf2_t vd,
                                                vfloat16mf2_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16m1_t __riscv_vfslide1down_vf_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                              _Float16 rs1, size_t vl);
vfloat16m2_t __riscv_vfslide1down_vf_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                              _Float16 rs1, size_t vl);
vfloat16m4_t __riscv_vfslide1down_vf_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                              _Float16 rs1, size_t vl);
vfloat16m8_t __riscv_vfslide1down_vf_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                              _Float16 rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_vf_f16mf4_tum(vbool64_t vm, vfloat16mf4_t vd,
                                               vfloat16mf4_t vs2, _Float16 rs1,
                                               size_t vl);
vfloat16mf2_t __riscv_vfslide1up_vf_f16mf2_tum(vbool32_t vm, vfloat16mf2_t vd,
                                               vfloat16mf2_t vs2, _Float16 rs1,
                                               size_t vl);
vfloat16m1_t __riscv_vfslide1up_vf_f16m1_tum(vbool16_t vm, vfloat16m1_t vd,
                                             vfloat16m1_t vs2, _Float16 rs1,
                                             size_t vl);
vfloat16m2_t __riscv_vfslide1up_vf_f16m2_tum(vbool8_t vm, vfloat16m2_t vd,
                                             vfloat16m2_t vs2, _Float16 rs1,
                                             size_t vl);
vfloat16m4_t __riscv_vfslide1up_vf_f16m4_tum(vbool4_t vm, vfloat16m4_t vd,
                                             vfloat16m4_t vs2, _Float16 rs1,
                                             size_t vl);
vfloat16m8_t __riscv_vfslide1up_vf_f16m8_tum(vbool2_t vm, vfloat16m8_t vd,
                                             vfloat16m8_t vs2, _Float16 rs1,
                                             size_t vl);
vfloat16mf4_t __riscv_vfslide1down_vf_f16mf4_tum(vbool64_t vm, vfloat16mf4_t vd,
                                                 vfloat16mf4_t vs2,
                                                 _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_vf_f16mf2_tum(vbool32_t vm, vfloat16mf2_t vd,
                                                 vfloat16mf2_t vs2,
                                                 _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1down_vf_f16m1_tum(vbool16_t vm, vfloat16m1_t vd,
                                               vfloat16m1_t vs2, _Float16 rs1,
                                               size_t vl);
vfloat16m2_t __riscv_vfslide1down_vf_f16m2_tum(vbool8_t vm, vfloat16m2_t vd,
                                               vfloat16m2_t vs2, _Float16 rs1,
                                               size_t vl);
vfloat16m4_t __riscv_vfslide1down_vf_f16m4_tum(vbool4_t vm, vfloat16m4_t vd,
                                               vfloat16m4_t vs2, _Float16 rs1,
                                               size_t vl);
vfloat16m8_t __riscv_vfslide1down_vf_f16m8_tum(vbool2_t vm, vfloat16m8_t vd,
                                               vfloat16m8_t vs2, _Float16 rs1,
                                               size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_vf_f16mf4_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                                vfloat16mf4_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16mf2_t __riscv_vfslide1up_vf_f16mf2_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                                vfloat16mf2_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16m1_t __riscv_vfslide1up_vf_f16m1_tumu(vbool16_t vm, vfloat16m1_t vd,
                                              vfloat16m1_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m2_t __riscv_vfslide1up_vf_f16m2_tumu(vbool8_t vm, vfloat16m2_t vd,
                                              vfloat16m2_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m4_t __riscv_vfslide1up_vf_f16m4_tumu(vbool4_t vm, vfloat16m4_t vd,
                                              vfloat16m4_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m8_t __riscv_vfslide1up_vf_f16m8_tumu(vbool2_t vm, vfloat16m8_t vd,
                                              vfloat16m8_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16mf4_t __riscv_vfslide1down_vf_f16mf4_tumu(vbool64_t vm,
                                                  vfloat16mf4_t vd,
                                                  vfloat16mf4_t vs2,
                                                  _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_vf_f16mf2_tumu(vbool32_t vm,
                                                  vfloat16mf2_t vd,
                                                  vfloat16mf2_t vs2,
                                                  _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1down_vf_f16m1_tumu(vbool16_t vm, vfloat16m1_t vd,
                                                vfloat16m1_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16m2_t __riscv_vfslide1down_vf_f16m2_tumu(vbool8_t vm, vfloat16m2_t vd,
                                                vfloat16m2_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16m4_t __riscv_vfslide1down_vf_f16m4_tumu(vbool4_t vm, vfloat16m4_t vd,
                                                vfloat16m4_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16m8_t __riscv_vfslide1down_vf_f16m8_tumu(vbool2_t vm, vfloat16m8_t vd,
                                                vfloat16m8_t vs2, _Float16 rs1,
                                                size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_vf_f16mf4_mu(vbool64_t vm, vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16mf2_t __riscv_vfslide1up_vf_f16mf2_mu(vbool32_t vm, vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m1_t __riscv_vfslide1up_vf_f16m1_mu(vbool16_t vm, vfloat16m1_t vd,
                                            vfloat16m1_t vs2, _Float16 rs1,
                                            size_t vl);
vfloat16m2_t __riscv_vfslide1up_vf_f16m2_mu(vbool8_t vm, vfloat16m2_t vd,
                                            vfloat16m2_t vs2, _Float16 rs1,
                                            size_t vl);
vfloat16m4_t __riscv_vfslide1up_vf_f16m4_mu(vbool4_t vm, vfloat16m4_t vd,
                                            vfloat16m4_t vs2, _Float16 rs1,
                                            size_t vl);
vfloat16m8_t __riscv_vfslide1up_vf_f16m8_mu(vbool2_t vm, vfloat16m8_t vd,
                                            vfloat16m8_t vs2, _Float16 rs1,
                                            size_t vl);
vfloat16mf4_t __riscv_vfslide1down_vf_f16mf4_mu(vbool64_t vm, vfloat16mf4_t vd,
                                                vfloat16mf4_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16mf2_t __riscv_vfslide1down_vf_f16mf2_mu(vbool32_t vm, vfloat16mf2_t vd,
                                                vfloat16mf2_t vs2, _Float16 rs1,
                                                size_t vl);
vfloat16m1_t __riscv_vfslide1down_vf_f16m1_mu(vbool16_t vm, vfloat16m1_t vd,
                                              vfloat16m1_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m2_t __riscv_vfslide1down_vf_f16m2_mu(vbool8_t vm, vfloat16m2_t vd,
                                              vfloat16m2_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m4_t __riscv_vfslide1down_vf_f16m4_mu(vbool4_t vm, vfloat16m4_t vd,
                                              vfloat16m4_t vs2, _Float16 rs1,
                                              size_t vl);
vfloat16m8_t __riscv_vfslide1down_vf_f16m8_mu(vbool2_t vm, vfloat16m8_t vd,
                                              vfloat16m8_t vs2, _Float16 rs1,
                                              size_t vl);
----

[[policy-variant-vector-register-gather]]
==== Vector Register Gather Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vrgather_vv_f16mf4_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                            vuint16mf4_t vs1, size_t vl);
vfloat16mf4_t __riscv_vrgather_vx_f16mf4_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                            size_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather_vv_f16mf2_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                            vuint16mf2_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather_vx_f16mf2_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                            size_t vs1, size_t vl);
vfloat16m1_t __riscv_vrgather_vv_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                          vuint16m1_t vs1, size_t vl);
vfloat16m1_t __riscv_vrgather_vx_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                          size_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather_vv_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                          vuint16m2_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather_vx_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                          size_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather_vv_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                          vuint16m4_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather_vx_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                          size_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather_vv_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                          vuint16m8_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather_vx_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                          size_t vs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_vv_f16mf4_tum(vbool64_t vm, vfloat16mf4_t vd,
                                             vfloat16mf4_t vs2,
                                             vuint16mf4_t vs1, size_t vl);
vfloat16mf4_t __riscv_vrgather_vx_f16mf4_tum(vbool64_t vm, vfloat16mf4_t vd,
                                             vfloat16mf4_t vs2, size_t vs1,
                                             size_t vl);
vfloat16mf2_t __riscv_vrgather_vv_f16mf2_tum(vbool32_t vm, vfloat16mf2_t vd,
                                             vfloat16mf2_t vs2,
                                             vuint16mf2_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather_vx_f16mf2_tum(vbool32_t vm, vfloat16mf2_t vd,
                                             vfloat16mf2_t vs2, size_t vs1,
                                             size_t vl);
vfloat16m1_t __riscv_vrgather_vv_f16m1_tum(vbool16_t vm, vfloat16m1_t vd,
                                           vfloat16m1_t vs2, vuint16m1_t vs1,
                                           size_t vl);
vfloat16m1_t __riscv_vrgather_vx_f16m1_tum(vbool16_t vm, vfloat16m1_t vd,
                                           vfloat16m1_t vs2, size_t vs1,
                                           size_t vl);
vfloat16m2_t __riscv_vrgather_vv_f16m2_tum(vbool8_t vm, vfloat16m2_t vd,
                                           vfloat16m2_t vs2, vuint16m2_t vs1,
                                           size_t vl);
vfloat16m2_t __riscv_vrgather_vx_f16m2_tum(vbool8_t vm, vfloat16m2_t vd,
                                           vfloat16m2_t vs2, size_t vs1,
                                           size_t vl);
vfloat16m4_t __riscv_vrgather_vv_f16m4_tum(vbool4_t vm, vfloat16m4_t vd,
                                           vfloat16m4_t vs2, vuint16m4_t vs1,
                                           size_t vl);
vfloat16m4_t __riscv_vrgather_vx_f16m4_tum(vbool4_t vm, vfloat16m4_t vd,
                                           vfloat16m4_t vs2, size_t vs1,
                                           size_t vl);
vfloat16m8_t __riscv_vrgather_vv_f16m8_tum(vbool2_t vm, vfloat16m8_t vd,
                                           vfloat16m8_t vs2, vuint16m8_t vs1,
                                           size_t vl);
vfloat16m8_t __riscv_vrgather_vx_f16m8_tum(vbool2_t vm, vfloat16m8_t vd,
                                           vfloat16m8_t vs2, size_t vs1,
                                           size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_vv_f16mf4_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2,
                                              vuint16mf4_t vs1, size_t vl);
vfloat16mf4_t __riscv_vrgather_vx_f16mf4_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                              vfloat16mf4_t vs2, size_t vs1,
                                              size_t vl);
vfloat16mf2_t __riscv_vrgather_vv_f16mf2_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2,
                                              vuint16mf2_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather_vx_f16mf2_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                              vfloat16mf2_t vs2, size_t vs1,
                                              size_t vl);
vfloat16m1_t __riscv_vrgather_vv_f16m1_tumu(vbool16_t vm, vfloat16m1_t vd,
                                            vfloat16m1_t vs2, vuint16m1_t vs1,
                                            size_t vl);
vfloat16m1_t __riscv_vrgather_vx_f16m1_tumu(vbool16_t vm, vfloat16m1_t vd,
                                            vfloat16m1_t vs2, size_t vs1,
                                            size_t vl);
vfloat16m2_t __riscv_vrgather_vv_f16m2_tumu(vbool8_t vm, vfloat16m2_t vd,
                                            vfloat16m2_t vs2, vuint16m2_t vs1,
                                            size_t vl);
vfloat16m2_t __riscv_vrgather_vx_f16m2_tumu(vbool8_t vm, vfloat16m2_t vd,
                                            vfloat16m2_t vs2, size_t vs1,
                                            size_t vl);
vfloat16m4_t __riscv_vrgather_vv_f16m4_tumu(vbool4_t vm, vfloat16m4_t vd,
                                            vfloat16m4_t vs2, vuint16m4_t vs1,
                                            size_t vl);
vfloat16m4_t __riscv_vrgather_vx_f16m4_tumu(vbool4_t vm, vfloat16m4_t vd,
                                            vfloat16m4_t vs2, size_t vs1,
                                            size_t vl);
vfloat16m8_t __riscv_vrgather_vv_f16m8_tumu(vbool2_t vm, vfloat16m8_t vd,
                                            vfloat16m8_t vs2, vuint16m8_t vs1,
                                            size_t vl);
vfloat16m8_t __riscv_vrgather_vx_f16m8_tumu(vbool2_t vm, vfloat16m8_t vd,
                                            vfloat16m8_t vs2, size_t vs1,
                                            size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_vv_f16mf4_mu(vbool64_t vm, vfloat16mf4_t vd,
                                            vfloat16mf4_t vs2, vuint16mf4_t vs1,
                                            size_t vl);
vfloat16mf4_t __riscv_vrgather_vx_f16mf4_mu(vbool64_t vm, vfloat16mf4_t vd,
                                            vfloat16mf4_t vs2, size_t vs1,
                                            size_t vl);
vfloat16mf2_t __riscv_vrgather_vv_f16mf2_mu(vbool32_t vm, vfloat16mf2_t vd,
                                            vfloat16mf2_t vs2, vuint16mf2_t vs1,
                                            size_t vl);
vfloat16mf2_t __riscv_vrgather_vx_f16mf2_mu(vbool32_t vm, vfloat16mf2_t vd,
                                            vfloat16mf2_t vs2, size_t vs1,
                                            size_t vl);
vfloat16m1_t __riscv_vrgather_vv_f16m1_mu(vbool16_t vm, vfloat16m1_t vd,
                                          vfloat16m1_t vs2, vuint16m1_t vs1,
                                          size_t vl);
vfloat16m1_t __riscv_vrgather_vx_f16m1_mu(vbool16_t vm, vfloat16m1_t vd,
                                          vfloat16m1_t vs2, size_t vs1,
                                          size_t vl);
vfloat16m2_t __riscv_vrgather_vv_f16m2_mu(vbool8_t vm, vfloat16m2_t vd,
                                          vfloat16m2_t vs2, vuint16m2_t vs1,
                                          size_t vl);
vfloat16m2_t __riscv_vrgather_vx_f16m2_mu(vbool8_t vm, vfloat16m2_t vd,
                                          vfloat16m2_t vs2, size_t vs1,
                                          size_t vl);
vfloat16m4_t __riscv_vrgather_vv_f16m4_mu(vbool4_t vm, vfloat16m4_t vd,
                                          vfloat16m4_t vs2, vuint16m4_t vs1,
                                          size_t vl);
vfloat16m4_t __riscv_vrgather_vx_f16m4_mu(vbool4_t vm, vfloat16m4_t vd,
                                          vfloat16m4_t vs2, size_t vs1,
                                          size_t vl);
vfloat16m8_t __riscv_vrgather_vv_f16m8_mu(vbool2_t vm, vfloat16m8_t vd,
                                          vfloat16m8_t vs2, vuint16m8_t vs1,
                                          size_t vl);
vfloat16m8_t __riscv_vrgather_vx_f16m8_mu(vbool2_t vm, vfloat16m8_t vd,
                                          vfloat16m8_t vs2, size_t vs1,
                                          size_t vl);
----

[[policy-variant-vector-compress]]
==== Vector Compress Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vcompress_vm_f16mf4_tu(vfloat16mf4_t vd,
                                             vfloat16mf4_t vs2, vbool64_t vs1,
                                             size_t vl);
vfloat16mf2_t __riscv_vcompress_vm_f16mf2_tu(vfloat16mf2_t vd,
                                             vfloat16mf2_t vs2, vbool32_t vs1,
                                             size_t vl);
vfloat16m1_t __riscv_vcompress_vm_f16m1_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                           vbool16_t vs1, size_t vl);
vfloat16m2_t __riscv_vcompress_vm_f16m2_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                           vbool8_t vs1, size_t vl);
vfloat16m4_t __riscv_vcompress_vm_f16m4_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                           vbool4_t vs1, size_t vl);
vfloat16m8_t __riscv_vcompress_vm_f16m8_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                           vbool2_t vs1, size_t vl);
----

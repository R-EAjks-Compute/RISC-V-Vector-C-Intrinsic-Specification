
=== Vector Permutation Intrinsics

[[overloaded-scalar-move]]
==== Integer and Floating-Point Scalar Move Intrinsics

[,c]
----
_Float16 __riscv_vfmv_f(vfloat16mf4_t vs1);
_Float16 __riscv_vfmv_f(vfloat16mf2_t vs1);
_Float16 __riscv_vfmv_f(vfloat16m1_t vs1);
_Float16 __riscv_vfmv_f(vfloat16m2_t vs1);
_Float16 __riscv_vfmv_f(vfloat16m4_t vs1);
_Float16 __riscv_vfmv_f(vfloat16m8_t vs1);
----

[[overloaded-vector-slideup]]
==== Vector Slideup Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vslideup(vfloat16mf4_t vd, vfloat16mf4_t vs2, size_t rs1,
                               size_t vl);
vfloat16mf2_t __riscv_vslideup(vfloat16mf2_t vd, vfloat16mf2_t vs2, size_t rs1,
                               size_t vl);
vfloat16m1_t __riscv_vslideup(vfloat16m1_t vd, vfloat16m1_t vs2, size_t rs1,
                              size_t vl);
vfloat16m2_t __riscv_vslideup(vfloat16m2_t vd, vfloat16m2_t vs2, size_t rs1,
                              size_t vl);
vfloat16m4_t __riscv_vslideup(vfloat16m4_t vd, vfloat16m4_t vs2, size_t rs1,
                              size_t vl);
vfloat16m8_t __riscv_vslideup(vfloat16m8_t vd, vfloat16m8_t vs2, size_t rs1,
                              size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup(vbool64_t vm, vfloat16mf4_t vd,
                               vfloat16mf4_t vs2, size_t rs1, size_t vl);
vfloat16mf2_t __riscv_vslideup(vbool32_t vm, vfloat16mf2_t vd,
                               vfloat16mf2_t vs2, size_t rs1, size_t vl);
vfloat16m1_t __riscv_vslideup(vbool16_t vm, vfloat16m1_t vd, vfloat16m1_t vs2,
                              size_t rs1, size_t vl);
vfloat16m2_t __riscv_vslideup(vbool8_t vm, vfloat16m2_t vd, vfloat16m2_t vs2,
                              size_t rs1, size_t vl);
vfloat16m4_t __riscv_vslideup(vbool4_t vm, vfloat16m4_t vd, vfloat16m4_t vs2,
                              size_t rs1, size_t vl);
vfloat16m8_t __riscv_vslideup(vbool2_t vm, vfloat16m8_t vd, vfloat16m8_t vs2,
                              size_t rs1, size_t vl);
----

[[overloaded-vector-slidedown]]
==== Vector Slidedown Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vslidedown(vfloat16mf4_t vs2, size_t rs1, size_t vl);
vfloat16mf2_t __riscv_vslidedown(vfloat16mf2_t vs2, size_t rs1, size_t vl);
vfloat16m1_t __riscv_vslidedown(vfloat16m1_t vs2, size_t rs1, size_t vl);
vfloat16m2_t __riscv_vslidedown(vfloat16m2_t vs2, size_t rs1, size_t vl);
vfloat16m4_t __riscv_vslidedown(vfloat16m4_t vs2, size_t rs1, size_t vl);
vfloat16m8_t __riscv_vslidedown(vfloat16m8_t vs2, size_t rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown(vbool64_t vm, vfloat16mf4_t vs2, size_t rs1,
                                 size_t vl);
vfloat16mf2_t __riscv_vslidedown(vbool32_t vm, vfloat16mf2_t vs2, size_t rs1,
                                 size_t vl);
vfloat16m1_t __riscv_vslidedown(vbool16_t vm, vfloat16m1_t vs2, size_t rs1,
                                size_t vl);
vfloat16m2_t __riscv_vslidedown(vbool8_t vm, vfloat16m2_t vs2, size_t rs1,
                                size_t vl);
vfloat16m4_t __riscv_vslidedown(vbool4_t vm, vfloat16m4_t vs2, size_t rs1,
                                size_t vl);
vfloat16m8_t __riscv_vslidedown(vbool2_t vm, vfloat16m8_t vs2, size_t rs1,
                                size_t vl);
----

[[overloaded-vector-slide1up-and-slide1down]]
==== Vector Slide1up and Slide1down Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vfslide1up(vfloat16mf4_t vs2, _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1up(vfloat16mf2_t vs2, _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1up(vfloat16m1_t vs2, _Float16 rs1, size_t vl);
vfloat16m2_t __riscv_vfslide1up(vfloat16m2_t vs2, _Float16 rs1, size_t vl);
vfloat16m4_t __riscv_vfslide1up(vfloat16m4_t vs2, _Float16 rs1, size_t vl);
vfloat16m8_t __riscv_vfslide1up(vfloat16m8_t vs2, _Float16 rs1, size_t vl);
vfloat16mf4_t __riscv_vfslide1down(vfloat16mf4_t vs2, _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1down(vfloat16mf2_t vs2, _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1down(vfloat16m1_t vs2, _Float16 rs1, size_t vl);
vfloat16m2_t __riscv_vfslide1down(vfloat16m2_t vs2, _Float16 rs1, size_t vl);
vfloat16m4_t __riscv_vfslide1down(vfloat16m4_t vs2, _Float16 rs1, size_t vl);
vfloat16m8_t __riscv_vfslide1down(vfloat16m8_t vs2, _Float16 rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up(vbool64_t vm, vfloat16mf4_t vs2, _Float16 rs1,
                                 size_t vl);
vfloat16mf2_t __riscv_vfslide1up(vbool32_t vm, vfloat16mf2_t vs2, _Float16 rs1,
                                 size_t vl);
vfloat16m1_t __riscv_vfslide1up(vbool16_t vm, vfloat16m1_t vs2, _Float16 rs1,
                                size_t vl);
vfloat16m2_t __riscv_vfslide1up(vbool8_t vm, vfloat16m2_t vs2, _Float16 rs1,
                                size_t vl);
vfloat16m4_t __riscv_vfslide1up(vbool4_t vm, vfloat16m4_t vs2, _Float16 rs1,
                                size_t vl);
vfloat16m8_t __riscv_vfslide1up(vbool2_t vm, vfloat16m8_t vs2, _Float16 rs1,
                                size_t vl);
vfloat16mf4_t __riscv_vfslide1down(vbool64_t vm, vfloat16mf4_t vs2,
                                   _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1down(vbool32_t vm, vfloat16mf2_t vs2,
                                   _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1down(vbool16_t vm, vfloat16m1_t vs2, _Float16 rs1,
                                  size_t vl);
vfloat16m2_t __riscv_vfslide1down(vbool8_t vm, vfloat16m2_t vs2, _Float16 rs1,
                                  size_t vl);
vfloat16m4_t __riscv_vfslide1down(vbool4_t vm, vfloat16m4_t vs2, _Float16 rs1,
                                  size_t vl);
vfloat16m8_t __riscv_vfslide1down(vbool2_t vm, vfloat16m8_t vs2, _Float16 rs1,
                                  size_t vl);
----

[[overloaded-vector-register-gather]]
==== Vector Register Gather Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vrgather(vfloat16mf4_t vs2, vuint16mf4_t vs1, size_t vl);
vfloat16mf4_t __riscv_vrgather(vfloat16mf4_t vs2, size_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather(vfloat16mf2_t vs2, vuint16mf2_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather(vfloat16mf2_t vs2, size_t vs1, size_t vl);
vfloat16m1_t __riscv_vrgather(vfloat16m1_t vs2, vuint16m1_t vs1, size_t vl);
vfloat16m1_t __riscv_vrgather(vfloat16m1_t vs2, size_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather(vfloat16m2_t vs2, vuint16m2_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather(vfloat16m2_t vs2, size_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather(vfloat16m4_t vs2, vuint16m4_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather(vfloat16m4_t vs2, size_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather(vfloat16m8_t vs2, vuint16m8_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather(vfloat16m8_t vs2, size_t vs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather(vbool64_t vm, vfloat16mf4_t vs2,
                               vuint16mf4_t vs1, size_t vl);
vfloat16mf4_t __riscv_vrgather(vbool64_t vm, vfloat16mf4_t vs2, size_t vs1,
                               size_t vl);
vfloat16mf2_t __riscv_vrgather(vbool32_t vm, vfloat16mf2_t vs2,
                               vuint16mf2_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather(vbool32_t vm, vfloat16mf2_t vs2, size_t vs1,
                               size_t vl);
vfloat16m1_t __riscv_vrgather(vbool16_t vm, vfloat16m1_t vs2, vuint16m1_t vs1,
                              size_t vl);
vfloat16m1_t __riscv_vrgather(vbool16_t vm, vfloat16m1_t vs2, size_t vs1,
                              size_t vl);
vfloat16m2_t __riscv_vrgather(vbool8_t vm, vfloat16m2_t vs2, vuint16m2_t vs1,
                              size_t vl);
vfloat16m2_t __riscv_vrgather(vbool8_t vm, vfloat16m2_t vs2, size_t vs1,
                              size_t vl);
vfloat16m4_t __riscv_vrgather(vbool4_t vm, vfloat16m4_t vs2, vuint16m4_t vs1,
                              size_t vl);
vfloat16m4_t __riscv_vrgather(vbool4_t vm, vfloat16m4_t vs2, size_t vs1,
                              size_t vl);
vfloat16m8_t __riscv_vrgather(vbool2_t vm, vfloat16m8_t vs2, vuint16m8_t vs1,
                              size_t vl);
vfloat16m8_t __riscv_vrgather(vbool2_t vm, vfloat16m8_t vs2, size_t vs1,
                              size_t vl);
----

[[overloaded-vector-compress]]
==== Vector Compress Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vcompress(vfloat16mf4_t vs2, vbool64_t vs1, size_t vl);
vfloat16mf2_t __riscv_vcompress(vfloat16mf2_t vs2, vbool32_t vs1, size_t vl);
vfloat16m1_t __riscv_vcompress(vfloat16m1_t vs2, vbool16_t vs1, size_t vl);
vfloat16m2_t __riscv_vcompress(vfloat16m2_t vs2, vbool8_t vs1, size_t vl);
vfloat16m4_t __riscv_vcompress(vfloat16m4_t vs2, vbool4_t vs1, size_t vl);
vfloat16m8_t __riscv_vcompress(vfloat16m8_t vs2, vbool2_t vs1, size_t vl);
----


=== Vector Permutation Intrinsics

[[scalar-move]]
==== Integer and Floating-Point Scalar Move Intrinsics

[,c]
----
_Float16 __riscv_vfmv_f_s_f16mf4_f16(vfloat16mf4_t vs1);
vfloat16mf4_t __riscv_vfmv_s_f_f16mf4(_Float16 rs1, size_t vl);
_Float16 __riscv_vfmv_f_s_f16mf2_f16(vfloat16mf2_t vs1);
vfloat16mf2_t __riscv_vfmv_s_f_f16mf2(_Float16 rs1, size_t vl);
_Float16 __riscv_vfmv_f_s_f16m1_f16(vfloat16m1_t vs1);
vfloat16m1_t __riscv_vfmv_s_f_f16m1(_Float16 rs1, size_t vl);
_Float16 __riscv_vfmv_f_s_f16m2_f16(vfloat16m2_t vs1);
vfloat16m2_t __riscv_vfmv_s_f_f16m2(_Float16 rs1, size_t vl);
_Float16 __riscv_vfmv_f_s_f16m4_f16(vfloat16m4_t vs1);
vfloat16m4_t __riscv_vfmv_s_f_f16m4(_Float16 rs1, size_t vl);
_Float16 __riscv_vfmv_f_s_f16m8_f16(vfloat16m8_t vs1);
vfloat16m8_t __riscv_vfmv_s_f_f16m8(_Float16 rs1, size_t vl);
----

[[vector-slideup]]
==== Vector Slideup Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vslideup_vx_f16mf4(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                         size_t rs1, size_t vl);
vfloat16mf2_t __riscv_vslideup_vx_f16mf2(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                         size_t rs1, size_t vl);
vfloat16m1_t __riscv_vslideup_vx_f16m1(vfloat16m1_t vd, vfloat16m1_t vs2,
                                       size_t rs1, size_t vl);
vfloat16m2_t __riscv_vslideup_vx_f16m2(vfloat16m2_t vd, vfloat16m2_t vs2,
                                       size_t rs1, size_t vl);
vfloat16m4_t __riscv_vslideup_vx_f16m4(vfloat16m4_t vd, vfloat16m4_t vs2,
                                       size_t rs1, size_t vl);
vfloat16m8_t __riscv_vslideup_vx_f16m8(vfloat16m8_t vd, vfloat16m8_t vs2,
                                       size_t rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslideup_vx_f16mf4_m(vbool64_t vm, vfloat16mf4_t vd,
                                           vfloat16mf4_t vs2, size_t rs1,
                                           size_t vl);
vfloat16mf2_t __riscv_vslideup_vx_f16mf2_m(vbool32_t vm, vfloat16mf2_t vd,
                                           vfloat16mf2_t vs2, size_t rs1,
                                           size_t vl);
vfloat16m1_t __riscv_vslideup_vx_f16m1_m(vbool16_t vm, vfloat16m1_t vd,
                                         vfloat16m1_t vs2, size_t rs1,
                                         size_t vl);
vfloat16m2_t __riscv_vslideup_vx_f16m2_m(vbool8_t vm, vfloat16m2_t vd,
                                         vfloat16m2_t vs2, size_t rs1,
                                         size_t vl);
vfloat16m4_t __riscv_vslideup_vx_f16m4_m(vbool4_t vm, vfloat16m4_t vd,
                                         vfloat16m4_t vs2, size_t rs1,
                                         size_t vl);
vfloat16m8_t __riscv_vslideup_vx_f16m8_m(vbool2_t vm, vfloat16m8_t vd,
                                         vfloat16m8_t vs2, size_t rs1,
                                         size_t vl);
----

[[vector-slidedown]]
==== Vector Slidedown Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vslidedown_vx_f16mf4(vfloat16mf4_t vs2, size_t rs1,
                                           size_t vl);
vfloat16mf2_t __riscv_vslidedown_vx_f16mf2(vfloat16mf2_t vs2, size_t rs1,
                                           size_t vl);
vfloat16m1_t __riscv_vslidedown_vx_f16m1(vfloat16m1_t vs2, size_t rs1,
                                         size_t vl);
vfloat16m2_t __riscv_vslidedown_vx_f16m2(vfloat16m2_t vs2, size_t rs1,
                                         size_t vl);
vfloat16m4_t __riscv_vslidedown_vx_f16m4(vfloat16m4_t vs2, size_t rs1,
                                         size_t vl);
vfloat16m8_t __riscv_vslidedown_vx_f16m8(vfloat16m8_t vs2, size_t rs1,
                                         size_t vl);
// masked functions
vfloat16mf4_t __riscv_vslidedown_vx_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                             size_t rs1, size_t vl);
vfloat16mf2_t __riscv_vslidedown_vx_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                             size_t rs1, size_t vl);
vfloat16m1_t __riscv_vslidedown_vx_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                           size_t rs1, size_t vl);
vfloat16m2_t __riscv_vslidedown_vx_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                           size_t rs1, size_t vl);
vfloat16m4_t __riscv_vslidedown_vx_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                           size_t rs1, size_t vl);
vfloat16m8_t __riscv_vslidedown_vx_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                           size_t rs1, size_t vl);
----

[[vector-slide1up-and-slide1down]]
==== Vector Slide1up and Slide1down Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vfslide1up_vf_f16mf4(vfloat16mf4_t vs2, _Float16 rs1,
                                           size_t vl);
vfloat16mf2_t __riscv_vfslide1up_vf_f16mf2(vfloat16mf2_t vs2, _Float16 rs1,
                                           size_t vl);
vfloat16m1_t __riscv_vfslide1up_vf_f16m1(vfloat16m1_t vs2, _Float16 rs1,
                                         size_t vl);
vfloat16m2_t __riscv_vfslide1up_vf_f16m2(vfloat16m2_t vs2, _Float16 rs1,
                                         size_t vl);
vfloat16m4_t __riscv_vfslide1up_vf_f16m4(vfloat16m4_t vs2, _Float16 rs1,
                                         size_t vl);
vfloat16m8_t __riscv_vfslide1up_vf_f16m8(vfloat16m8_t vs2, _Float16 rs1,
                                         size_t vl);
vfloat16mf4_t __riscv_vfslide1down_vf_f16mf4(vfloat16mf4_t vs2, _Float16 rs1,
                                             size_t vl);
vfloat16mf2_t __riscv_vfslide1down_vf_f16mf2(vfloat16mf2_t vs2, _Float16 rs1,
                                             size_t vl);
vfloat16m1_t __riscv_vfslide1down_vf_f16m1(vfloat16m1_t vs2, _Float16 rs1,
                                           size_t vl);
vfloat16m2_t __riscv_vfslide1down_vf_f16m2(vfloat16m2_t vs2, _Float16 rs1,
                                           size_t vl);
vfloat16m4_t __riscv_vfslide1down_vf_f16m4(vfloat16m4_t vs2, _Float16 rs1,
                                           size_t vl);
vfloat16m8_t __riscv_vfslide1down_vf_f16m8(vfloat16m8_t vs2, _Float16 rs1,
                                           size_t vl);
// masked functions
vfloat16mf4_t __riscv_vfslide1up_vf_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                             _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1up_vf_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                             _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1up_vf_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                           _Float16 rs1, size_t vl);
vfloat16m2_t __riscv_vfslide1up_vf_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                           _Float16 rs1, size_t vl);
vfloat16m4_t __riscv_vfslide1up_vf_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                           _Float16 rs1, size_t vl);
vfloat16m8_t __riscv_vfslide1up_vf_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                           _Float16 rs1, size_t vl);
vfloat16mf4_t __riscv_vfslide1down_vf_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                               _Float16 rs1, size_t vl);
vfloat16mf2_t __riscv_vfslide1down_vf_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                               _Float16 rs1, size_t vl);
vfloat16m1_t __riscv_vfslide1down_vf_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                             _Float16 rs1, size_t vl);
vfloat16m2_t __riscv_vfslide1down_vf_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                             _Float16 rs1, size_t vl);
vfloat16m4_t __riscv_vfslide1down_vf_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                             _Float16 rs1, size_t vl);
vfloat16m8_t __riscv_vfslide1down_vf_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                             _Float16 rs1, size_t vl);
----

[[vector-register-gather]]
==== Vector Register Gather Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vrgather_vv_f16mf4(vfloat16mf4_t vs2, vuint16mf4_t vs1,
                                         size_t vl);
vfloat16mf4_t __riscv_vrgather_vx_f16mf4(vfloat16mf4_t vs2, size_t vs1,
                                         size_t vl);
vfloat16mf2_t __riscv_vrgather_vv_f16mf2(vfloat16mf2_t vs2, vuint16mf2_t vs1,
                                         size_t vl);
vfloat16mf2_t __riscv_vrgather_vx_f16mf2(vfloat16mf2_t vs2, size_t vs1,
                                         size_t vl);
vfloat16m1_t __riscv_vrgather_vv_f16m1(vfloat16m1_t vs2, vuint16m1_t vs1,
                                       size_t vl);
vfloat16m1_t __riscv_vrgather_vx_f16m1(vfloat16m1_t vs2, size_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather_vv_f16m2(vfloat16m2_t vs2, vuint16m2_t vs1,
                                       size_t vl);
vfloat16m2_t __riscv_vrgather_vx_f16m2(vfloat16m2_t vs2, size_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather_vv_f16m4(vfloat16m4_t vs2, vuint16m4_t vs1,
                                       size_t vl);
vfloat16m4_t __riscv_vrgather_vx_f16m4(vfloat16m4_t vs2, size_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather_vv_f16m8(vfloat16m8_t vs2, vuint16m8_t vs1,
                                       size_t vl);
vfloat16m8_t __riscv_vrgather_vx_f16m8(vfloat16m8_t vs2, size_t vs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_vrgather_vv_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                           vuint16mf4_t vs1, size_t vl);
vfloat16mf4_t __riscv_vrgather_vx_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                           size_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather_vv_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                           vuint16mf2_t vs1, size_t vl);
vfloat16mf2_t __riscv_vrgather_vx_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                           size_t vs1, size_t vl);
vfloat16m1_t __riscv_vrgather_vv_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                         vuint16m1_t vs1, size_t vl);
vfloat16m1_t __riscv_vrgather_vx_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                         size_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather_vv_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                         vuint16m2_t vs1, size_t vl);
vfloat16m2_t __riscv_vrgather_vx_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                         size_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather_vv_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                         vuint16m4_t vs1, size_t vl);
vfloat16m4_t __riscv_vrgather_vx_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                         size_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather_vv_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                         vuint16m8_t vs1, size_t vl);
vfloat16m8_t __riscv_vrgather_vx_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                         size_t vs1, size_t vl);
----

[[vector-compress]]
==== Vector Compress Intrinsics

[,c]
----
vfloat16mf4_t __riscv_vcompress_vm_f16mf4(vfloat16mf4_t vs2, vbool64_t vs1,
                                          size_t vl);
vfloat16mf2_t __riscv_vcompress_vm_f16mf2(vfloat16mf2_t vs2, vbool32_t vs1,
                                          size_t vl);
vfloat16m1_t __riscv_vcompress_vm_f16m1(vfloat16m1_t vs2, vbool16_t vs1,
                                        size_t vl);
vfloat16m2_t __riscv_vcompress_vm_f16m2(vfloat16m2_t vs2, vbool8_t vs1,
                                        size_t vl);
vfloat16m4_t __riscv_vcompress_vm_f16m4(vfloat16m4_t vs2, vbool4_t vs1,
                                        size_t vl);
vfloat16m8_t __riscv_vcompress_vm_f16m8(vfloat16m8_t vs2, vbool2_t vs1,
                                        size_t vl);
----
